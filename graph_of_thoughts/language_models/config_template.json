{
    "gemini-1.5" : {
        "model_id": "gemini-1.5-pro-preview-0409",
        "prompt_token_cost": 0.000125,
        "response_token_cost": 0.000375,
        "temperature": 0.9, 
        "top_k": 32,
        "top_p": 1.0, 
        "candidate_count": 1, 
        "max_output_tokens": 8192,
        "project": "sota-generator",
        "location": "europe-southwest1"
    },
    "gemini-1.0" : {
        "model_id": "gemini-1.0-pro",
        "prompt_token_cost": 0.000125,
        "response_token_cost": 0.000375,
        "temperature": 0.9, 
        "top_k": 32,
        "top_p": 1.0, 
        "candidate_count": 1, 
        "max_output_tokens": 8192,
        "project": "sota-generator",
        "location": "europe-southwest1"
    },
    "chatgpt" : {
        "model_id": "gpt-3.5-turbo-0125",
        "prompt_token_cost": 0.0005,
        "response_token_cost": 0.0015,
        "temperature": 1.20,
        "max_tokens": 4096,
        "stop": null,
        "organization": "org-F6vhnlt774ZUjV95Y9h8WQF8",
        "api_key": ""
    },
    "gpt-3.5-turbo-0125" : {
        "model_id": "gpt-3.5-turbo-0125",
        "prompt_token_cost": 0.0005,
        "response_token_cost": 0.0015,
        "temperature": 1.20,
        "max_tokens": 4096,
        "stop": null,
        "organization": "org-F6vhnlt774ZUjV95Y9h8WQF8",
        "api_key": ""
    },
    "gpt-4-0125-preview" : {
        "model_id": "gpt-4-0125-preview",
        "prompt_token_cost": 0.01,
        "response_token_cost": 0.03,
        "temperature": 1.0,
        "max_tokens": 4096,
        "stop": null,
        "organization": "org-F6vhnlt774ZUjV95Y9h8WQF8",
        "api_key": ""
    },
    "llama7b-hf" : {
        "model_id": "Llama-2-7b-chat-hf",
        "cache_dir": "./llama",
        "prompt_token_cost": 0.0,
        "response_token_cost": 0.0,
        "temperature": 0.6,
        "top_k": 10,
        "max_tokens": 2048
    },
    "llama13b-hf" : {
        "model_id": "Llama-2-13b-chat-hf",
        "cache_dir": "./llama",
        "prompt_token_cost": 0.0,
        "response_token_cost": 0.0,
        "temperature": 0.6,
        "top_k": 10,
        "max_tokens": 4096
    },
    "llama70b-hf" : {
        "model_id": "Llama-2-70b-chat-hf",
        "cache_dir": "./llama",
        "prompt_token_cost": 0.0,
        "response_token_cost": 0.0,
        "temperature": 0.6,
        "top_k": 10,
        "max_tokens": 4096
    }
}
